{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Libraries"
      ],
      "metadata": {
        "id": "4Me6UumQrBPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "KxCsrcSuAfkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "Lqdt4GimBOtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "_ttmX_z4BZ0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "JCgpxhJfB-BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bsojLaEzMKMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "HfzUZjr4U9Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis."
      ],
      "metadata": {
        "id": "4ueag_kVa8Zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot without Chat_Model"
      ],
      "metadata": {
        "id": "8GfLrf9VrQhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It doesn't have information about the outside world.  "
      ],
      "metadata": {
        "id": "Tnd-EjzpASC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HVSRZiaPATUj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "Bj6i2IKbAloB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"When Allah's messenger feel afraid?\"\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxkMJS8HAqXJ",
        "outputId": "a2cfe048-114d-49fb-a728-ecbef8fa7ef1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When Allah's messenger feel afraid?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text document\n",
        "loader = TextLoader(\"/content/hadith_output16.txt\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader]) #'from_loaders' method is likely responsible for creating an index or data structure that allows for efficient storage and retrieval of text data, possibly for tasks like searching, querying, or analysis."
      ],
      "metadata": {
        "id": "mUL1tfkxAr1r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines of code suggest that we are loading and processing text data from the file \"hadith_output16.txt\" using the TextLoader class, and then creating an index or data structure using the VectorstoreIndexCreator class to enable efficient handling of the text data"
      ],
      "metadata": {
        "id": "B5LlhmmQE04J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here 'index' sorts and arranges the text in a way that makes it easier to find what we're looking for, just like an index in a book helps us quickly find the pages related to a specific topic."
      ],
      "metadata": {
        "id": "yjk9Iw1kIXWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the query\n",
        "results = index.query(query)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixHDAi0YAudk",
        "outputId": "d32f8f70-f139-4388-816d-d2ad6338dcd5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Allah's Messenger (ï·º) felt afraid when he received the revelation from Allah and returned with his heart beating severely.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use 'results = index.query(query)', for asking this smart 'index' to find information based on a question or keyword provide in the query. The results are like the answers or information that the smart index found for us based on our question.\n",
        "\n",
        "Think of it like a librarian who has carefully organized all the books in a library and can quickly find the right ones when we ask a question. The 'index' is like that librarian, and the 'query' is your question."
      ],
      "metadata": {
        "id": "qqwTFM7KI8_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot With llm"
      ],
      "metadata": {
        "id": "EbLyhIK2U-5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two different data pipelines. They either queries our own personal data or the llm model.  "
      ],
      "metadata": {
        "id": "4b55EznA-Fsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://blog.langchain.dev/content/images/2023/03/image-1.png\">\n",
        "First LLM is going to take in the chat history / new question. Then it's going to create a new Standalone Question and it's going to send this question to either the LLM model/ the vector store which contains our own personal data and then it's going to try to combine these together and give us an answer.  "
      ],
      "metadata": {
        "id": "D9c0EHuJ9xzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "0etzU-4BU9U5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"If I hurt poor people, is it good in islam? Show me the references\""
      ],
      "metadata": {
        "id": "nvgsQ1ceVFpV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the query\n",
        "results = index.query(query, llm=ChatOpenAI())\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9p11uceVUnz",
        "outputId": "58af53ed-455d-4772-d418-578e1d27e183"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Islam, it is not good to harm anyone, including poor people. The Prophet Muhammad (peace be upon him) emphasized the importance of treating others with kindness and avoiding harm. The following references highlight the importance of good deeds and treating others well:\n",
            "\n",
            "1. Sahih al-Bukhari 11:\n",
            "Narrated by Abdullah bin 'Amr: The Prophet (peace be upon him) said, \"One who avoids harming the Muslims with his tongue and hands is the best in Islam.\"\n",
            "\n",
            "2. Sahih al-Bukhari 12:\n",
            "Narrated by Anas: The Prophet (peace be upon him) said, \"None of you will have faith until he wishes for his Muslim brother what he likes for himself.\"\n",
            "\n",
            "3. Sahih al-Bukhari 28:\n",
            "Narrated by Abu Huraira: A person asked the Prophet (peace be upon him) about good deeds in Islam. He replied, \"To feed the poor and greet those whom you know and those whom you don't know.\"\n",
            "\n",
            "These references emphasize the importance of treating others with kindness, including the poor, and avoiding harm towards them. Hurting or causing harm to anyone, especially those who are less fortunate, goes against the teachings of Islam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API data usage policies\n",
        "Starting on March 1, 2023, we are making two changes to our data usage and retention policies:\n",
        "\n",
        "<li>OpenAI will not use data submitted by customers via our API to train or improve our models, unless you explicitly decide to share your data with us for this purpose. You can opt-in to share data.\n",
        "<li>Any data sent through the API will be retained for abuse and misuse monitoring purposes for a maximum of 30 days, after which it will be deleted (unless otherwise required by law)."
      ],
      "metadata": {
        "id": "1gVo4Y0QDfQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we used OpenAI model?\n",
        "For building a content-based question-answering chatbot, both NLP and GPT can be used, but the choice depends on our specific requirements and resources. Let's consider the strengths and use cases of each approach:\n",
        "\n",
        "####NLP-based Approach:\n",
        "<b>Control:</b> NLP-based models allow us more control over the responses and logic of the chatbot. we can define specific rules and patterns for extracting information from user queries and providing accurate answers.\n",
        "\n",
        "<b>Domain Expertise:</b> If we have a well-defined domain with specific terminology and rules, an NLP-based approach can be effective in capturing and leveraging that domain knowledge.\n",
        "\n",
        "<b>Efficiency:</b> NLP-based models can be efficient and lightweight, making them suitable for scenarios where we want quick and specific answers.\n",
        "\n",
        "####GPT-based Approach:\n",
        "\n",
        "<b>Natural Language Understanding:</b> GPT models excel at understanding context and generating human-like responses. They can handle a wide range of language variations and conversational nuances.\n",
        "\n",
        "<b>Generalization:</b> GPT models can generalize well across different types of content and topics. If our chatbot needs to handle diverse questions and discussions, GPT can be more flexible.\n",
        "\n",
        "<b>Engagement:</b> GPT-based chatbots often provide more engaging and interactive conversations due to their ability to generate coherent and contextually relevant responses.\n",
        "\n",
        "<b>Scalability:</b> GPT-based models like ChatGPT can easily handle a variety of content and questions without requiring extensive rule-building.\n",
        "\n",
        "In general, if we have a well-defined domain with specific content and structured data, an NLP-based approach might be suitable. On the other hand, if we want a more conversational and engaging chatbot that can handle a wider variety of questions, a GPT-based approach like ChatGPT can be a good choice."
      ],
      "metadata": {
        "id": "ErVf23VdLPaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find hadith randomly and summarize that\n",
        "\n"
      ],
      "metadata": {
        "id": "l_bT2-_IJ_ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.generic_utils import default\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "import random\n",
        "\n",
        "classifier = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
        "output = ''\n",
        "k = random.randint(1, 16)\n",
        "x = str(k)\n",
        "html_file = requests.get(\"https://sunnah.com/bukhari/\" + x).text\n",
        "\n",
        "soup = BeautifulSoup(html_file, 'lxml')\n",
        "\n",
        "hadith_narrated = soup.find_all('div', class_=\"hadith_narrated\")\n",
        "hadith = soup.find_all('div', class_=\"text_details\")\n",
        "reference = soup.find_all('table', class_=\"hadith_reference\")\n",
        "\n",
        "p = len(hadith_narrated)\n",
        "q = len(hadith)\n",
        "r = len(reference)\n",
        "minimum = min(p,q,r)\n",
        "\n",
        "i = random.randint(0, minimum-1)\n",
        "a = hadith_narrated[i].text\n",
        "b = hadith[i].text\n",
        "c = reference[i].tr.a.text\n",
        "output += f'Hadith Narrated by: {a}\\n'\n",
        "output += f'Hadith: {b}\\n'\n",
        "output += f'Reference: {c}\\n\\n'\n",
        "print(output)\n",
        "s = classifier(output)\n",
        "summarize = s[0]['summary_text']\n",
        "print(\"Summerization of the result\")\n",
        "print(summarize)"
      ],
      "metadata": {
        "id": "Xg8vJ299KtCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1645fbd-56be-4005-f477-130733ada618"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadith Narrated by: \n",
            "Narrated Ibn Abu Mulaika:\n",
            "Hadith: Whenever `Aisha (the wife of the Prophet) heard anything which she did not understand, she used to \n",
            "ask again till she understood it completely. Aisha said: \"Once the Prophet (ï·º) said, \"Whoever will be \n",
            "called to account (about his deeds on the Day of Resurrection) will surely be punished.\" I said, \n",
            "\"Doesn't Allah say: \"He surely will receive an easy reckoning.\" (84.8) The Prophet (ï·º) replied, \"This \n",
            "means only the presentation of the accounts but whoever will be argued about his account, will \n",
            "certainly be ruined.\"\n",
            "Reference: Sahih al-Bukhari 103\n",
            "\n",
            "\n",
            "Summerization of the result\n",
            " The Prophet (ï·º) said, \"Whoever will be  called to account (about his deeds on the Day of Resurrection) will surely be punished\" Aisha said: \"Doesn't Allah say: \"He surely will receive an easy reckoning.\" (84.8) The Prophet replied, \"This means only the presentation of the accounts but whoever will be argued about his account, will certainly be ruined\"Reference: Sahih al-Bukhari 103-103 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summerization"
      ],
      "metadata": {
        "id": "oZE7jcq5vCm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
        "s = classifier(output)\n",
        "summarize = s[0]['summary_text']\n",
        "summarize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "h4XS238NLRsb",
        "outputId": "4e53aa66-e16e-4d5e-a251-f05c95d76c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Allah\\'s Messenger (ï·º) (p.b.u.h) offered the Fajr prayer when it was still dark, then he rode and said, \\'Allah  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Akbar! Khaibar is ruined.\\' The people came out into the streets saying, \"Muhammad and his \\xa0army\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web App"
      ],
      "metadata": {
        "id": "fTr6svY4vH6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Hadith Generator"
      ],
      "metadata": {
        "id": "op7qRO9kPPbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import random\n",
        "\n",
        "def generate_hadith():\n",
        "    output = \"\"\n",
        "    k = random.randint(1, 16)\n",
        "    x = str(k)\n",
        "    html_file = requests.get(\"https://sunnah.com/bukhari/\" + x).text\n",
        "\n",
        "    soup = BeautifulSoup(html_file, 'lxml')\n",
        "\n",
        "    hadith_narrated = soup.find_all('div', class_=\"hadith_narrated\")\n",
        "    hadith = soup.find_all('div', class_=\"text_details\")\n",
        "    reference = soup.find_all('table', class_=\"hadith_reference\")\n",
        "\n",
        "    p = len(hadith_narrated)\n",
        "    q = len(hadith)\n",
        "    r = len(reference)\n",
        "    minimum = min(p, q, r)\n",
        "\n",
        "    i = random.randint(0, minimum-1)\n",
        "    a = hadith_narrated[i].text\n",
        "    b = hadith[i].text\n",
        "    c = reference[i].tr.a.text\n",
        "    output += f'Hadith Narrated by: {a}\\n'\n",
        "    output += f'Hadith: {b}\\n'\n",
        "    output += f'Reference: {c}\\n\\n'\n",
        "    return output\n",
        "\n",
        "def generate_output():\n",
        "    output = generate_hadith()\n",
        "    return output\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_output,\n",
        "    inputs=None,\n",
        "    outputs=\"text\",\n",
        "    title=\"Random Hadith Generator\",\n",
        "    description=\"Click the button to generate a random hadith.\",\n",
        "    theme=\"default\",\n",
        "    layout=\"centered\",\n",
        "    button_text=\"Generate Hadith\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "K0fobMbEPOge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summeraize"
      ],
      "metadata": {
        "id": "5d51srW8dfDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_summary(text):\n",
        "    classifier = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
        "    s = classifier(text)\n",
        "    summarize = s[0]['summary_text']\n",
        "    return summarize\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_summary,\n",
        "    inputs=\"textbox\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Text Summarizer\",\n",
        "    description=\"Enter the text to summarize:\",\n",
        "    theme=\"default\",\n",
        "    layout=\"vertical\",\n",
        "    width=\"500px\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "Aan-kMZceeXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering UI"
      ],
      "metadata": {
        "id": "y8HJaG2DGj3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Load the text document and create the index\n",
        "loader = TextLoader(\"/content/hadith_output16.txt\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "def query_results(query):\n",
        "    if query.lower() == \"who are you?\":\n",
        "        response = \"I am a Islamic chatbot.\"\n",
        "    elif query.lower() == \"who made you?\":\n",
        "        response = \"Iftakhir Nibir and Noor Safa made me\"\n",
        "    elif query.lower() == \"how are you?\":\n",
        "        response = \"Alhamdulilha, Thanks for asking. How can I help you today?\"\n",
        "    elif query.lower() == \"hello\":\n",
        "        response = \"As Salamu Walaikum, How can I help you to know about Islam?\"\n",
        "    else:\n",
        "        # Perform the query using the preloaded index\n",
        "        results = index.query(query)\n",
        "        response = results\n",
        "\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=query_results,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Islamic Conversation\",\n",
        "    description=\"Enter your question to retrieve results.\",\n",
        "    theme=\"sketchy\",\n",
        "    layout=\"vertically\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "DDbKC9sh5QXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot UI System"
      ],
      "metadata": {
        "id": "j4HZLQSwGemh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Load the text document and create the index\n",
        "loader = TextLoader(\"/content/hadith_output16.txt\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "def query_results(query):\n",
        "    if query.lower() == \"who are you?\":\n",
        "        response = \"I am a Islamic chatbot.\"\n",
        "    elif query.lower() == \"who made you?\":\n",
        "        response = \"Iftakhir Nibir and Noor Safa made me\"\n",
        "    elif query.lower() == \"how are you?\":\n",
        "        response = \"Alhamdulilha, Thanks for asking. How can I help you today?\"\n",
        "    elif query.lower() == \"hello\":\n",
        "        response = \"As Salamu Walaikum, How can I help you to know about Islam?\"\n",
        "    else:\n",
        "        # Perform the query using the preloaded index\n",
        "        results = index.query(query)\n",
        "        response = results\n",
        "\n",
        "    return response\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_message = query_results(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "WwJVHasPO89Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final APP   "
      ],
      "metadata": {
        "id": "tKjWXfU9GUNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import random\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Load the text document and create the index\n",
        "loader = TextLoader(\"/content/hadith_output16.txt\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "def query_results(query):\n",
        "    if query.lower() == \"who are you?\":\n",
        "        response = \"I am a Islamic chatbot.\"\n",
        "    elif query.lower() == \"who made you?\":\n",
        "        response = \"I was made by Iftakhir Nibir ðââï¸ and his team members\"\n",
        "    elif query.lower() == \"who are the members of iftakhir nibir and his team?\":\n",
        "        response = \"They are nothing but human beings ðââï¸ðââï¸ðââï¸ðââï¸.Those who want to get permanent job ðð\"\n",
        "    elif query.lower() == \"how are you?\":\n",
        "        response = \"Alhamdulilha, Thanks for asking. How can I help you today?\"\n",
        "    elif query.lower() == \"hello\":\n",
        "        response = random.choice([\"As Salamu Walaikum , How can I help you to know about Islam?\", \"As Salamu Alaikum Warahmatullahi Wabarakatuh\"])\n",
        "    else:\n",
        "        # Perform the query using the preloaded index\n",
        "        results = index.query(query)\n",
        "        response = results\n",
        "\n",
        "    return response\n",
        "\n",
        "def query_results_with_chat_model(query):\n",
        "    if query.lower() == \"who are you?\":\n",
        "        response = \"I am a Islamic chatbot.\"\n",
        "    elif query.lower() == \"who made you?\":\n",
        "        response = \"I was made by Iftakhir Nibir ðââï¸ and his team members\"\n",
        "    elif query.lower() == \"who are the members of iftakhir nibir and his team?\":\n",
        "        response = \"They are nothing but human beings ðââï¸ðââï¸ðââï¸ðââï¸.Those who want to get permanent job ðð\"\n",
        "    elif query.lower() == \"how are you?\":\n",
        "        response = \"Alhamdulilha, Thanks for asking. How can I help you today?\"\n",
        "    elif query.lower() == \"hello\":\n",
        "        response = random.choice([\"As Salamu Walaikum , How can I help you to know about Islam?\", \"As Salamu Alaikum Warahmatullahi Wabarakatuh\"])\n",
        "    else:\n",
        "        # Perform the query using the preloaded index\n",
        "        results = index.query(query, llm=ChatOpenAI())\n",
        "        response = results\n",
        "\n",
        "    return response\n",
        "\n",
        "def generate_hadith():\n",
        "    output = \"\"\n",
        "    k = random.randint(1, 16)\n",
        "    x = str(k)\n",
        "    html_file = requests.get(\"https://sunnah.com/bukhari/\" + x).text\n",
        "\n",
        "    soup = BeautifulSoup(html_file, 'lxml')\n",
        "\n",
        "    hadith_narrated = soup.find_all('div', class_=\"hadith_narrated\")\n",
        "    hadith = soup.find_all('div', class_=\"text_details\")\n",
        "    reference = soup.find_all('table', class_=\"hadith_reference\")\n",
        "\n",
        "    p = len(hadith_narrated)\n",
        "    q = len(hadith)\n",
        "    r = len(reference)\n",
        "    minimum = min(p, q, r)\n",
        "\n",
        "    i = random.randint(0, minimum-1)\n",
        "    a = hadith_narrated[i].text\n",
        "    b = hadith[i].text\n",
        "    c = reference[i].tr.a.text\n",
        "    output += f'Hadith Narrated by: {a}\\n'\n",
        "    output += f'Hadith: {b}\\n'\n",
        "    output += f'Reference: {c}\\n\\n'\n",
        "    return output\n",
        "\n",
        "def generate_output():\n",
        "    output = generate_hadith()\n",
        "    return output\n",
        "\n",
        "def generate_summary(text):\n",
        "    classifier = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
        "    s = classifier(text)\n",
        "    summarize = s[0]['summary_text']\n",
        "    return summarize\n",
        "\n",
        "\n",
        "def flip_text(x):\n",
        "    return x[::-1]\n",
        "\n",
        "\n",
        "def flip_image(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "def flip_audio(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "def flip_typing(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"Islamic AI System\")\n",
        "    with gr.Tab(\"Chatbot:\"):\n",
        "        chatbot = gr.Chatbot()\n",
        "        msg = gr.Textbox()\n",
        "        clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "        def respond(message, chat_history):\n",
        "            bot_message = query_results(message)\n",
        "            chat_history.append((message, bot_message))\n",
        "            return \"\", chat_history\n",
        "\n",
        "    with gr.Tab(\"OpenAI based Chatmodel:\"):\n",
        "        chatbot2 = gr.Chatbot()\n",
        "        msg2 = gr.Textbox()\n",
        "        clear = gr.ClearButton([msg2, chatbot2])\n",
        "\n",
        "        def respond2(message, chat_history):\n",
        "            bot_message = query_results_with_chat_model(message)\n",
        "            chat_history.append((message, bot_message))\n",
        "            return \"\", chat_history\n",
        "\n",
        "    with gr.Tab(\"Read Hadith\"):\n",
        "        iface = gr.Interface(\n",
        "        fn=generate_output,\n",
        "        inputs=None,\n",
        "        outputs=\"text\",\n",
        "        title=\"Read Hadith Daily\",\n",
        "        description=\"Click the button to get a Hadith.\",\n",
        "        theme=\"default\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"Hadith Summarization\"):\n",
        "        iface = gr.Interface(\n",
        "        fn=generate_summary,\n",
        "        inputs=\"textbox\",\n",
        "        outputs=\"text\",\n",
        "        title=\"Hadith Summarizer\",\n",
        "        description=\"Enter the hadith to summarize:\",\n",
        "        theme=\"default\"\n",
        "    )\n",
        "\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    msg2.submit(respond2, [msg2, chatbot2], [msg2, chatbot2])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "6bi7MQQr1ykj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}